{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 1\n",
    "SEEDS = torch.arange(SEED_NUM)\n",
    "EPOCHS = 10\n",
    "OPTIMIZERS = {\n",
    "    'SGD': optim.SGD,\n",
    "    'Adam': optim.Adam,\n",
    "    'RMSprop': optim.RMSprop,\n",
    "    \"AdaGrad\": optim.Adagrad,\n",
    "    \"AMSGrad\": optim.Adam\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(3*32*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainval(train_loader, test_loader, model, optimizer, criterion, device, epochs=10):\n",
    "    # Train the model\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        model.train()\n",
    "        for (images, labels) in tqdm(train_loader, leave=False, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the training loss\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        # print(f\"Epoch {epoch+1} train loss: {epoch_train_loss / len(train_loader)}\")\n",
    "        train_loss.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "        # print(f\"Epoch {epoch+1} validation loss: {epoch_val_loss / len(test_loader)}\")\n",
    "        val_loss.append(epoch_val_loss / len(test_loader))\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def test(test_loader, model, device):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get the predicted class for each image\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store the true and predicted labels\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Create data loaders for the training and testing datasets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = {}\n",
    "val_losses = {}\n",
    "accuracy = {}\n",
    "for optimizer_name in OPTIMIZERS:\n",
    "    print(\"Running optimizer:\", optimizer_name)\n",
    "    train_losses[optimizer_name] = []\n",
    "    val_losses[optimizer_name] = []\n",
    "    for seed in SEEDS:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Instantiate the logistic regression model\n",
    "        model = LogisticRegression()\n",
    "        model.to(device)\n",
    "\n",
    "        # Define the optimizer and the loss function\n",
    "        if optimizer_name == \"AMSGrad\":\n",
    "            optimizer = OPTIMIZERS[optimizer_name](model.parameters(), lr=0.001, amsgrad=True)\n",
    "        else:\n",
    "            optimizer = OPTIMIZERS[optimizer_name](model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loss, val_loss = trainval(train_loader, test_loader, model, optimizer, criterion, device, epochs=EPOCHS)\n",
    "        train_losses[optimizer_name].append(train_loss)\n",
    "        val_losses[optimizer_name].append(val_loss)\n",
    "\n",
    "        # Test the model\n",
    "        acc = test(test_loader, model, device)\n",
    "        accuracy[optimizer_name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(EPOCHS)\n",
    "x = np.tile(x, SEED_NUM)\n",
    "for optimizer_name in OPTIMIZERS:\n",
    "    y = np.concatenate(train_losses[optimizer_name])\n",
    "    sns.lineplot(x=x, y=y, label=optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(EPOCHS)\n",
    "x = np.tile(x, SEED_NUM)\n",
    "for optimizer_name in OPTIMIZERS:\n",
    "    y = np.concatenate(val_losses[optimizer_name])\n",
    "    sns.lineplot(x=x, y=y, label=optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
