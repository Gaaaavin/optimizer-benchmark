{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 5\n",
    "SEEDS = torch.arange(SEED_NUM)\n",
    "EPOCHS = 100\n",
    "OPTIMIZERS = {\n",
    "    'SGD': optim.SGD,\n",
    "    'Adam': optim.Adam,\n",
    "    'RMSprop': optim.RMSprop,\n",
    "    \"AdaGrad\": optim.Adagrad,\n",
    "    \"AMSGrad\": optim.Adam\n",
    "}\n",
    "# OPTIMIZERS = {\n",
    "#     'Adam': optim.Adam,\n",
    "#     \"AMSGrad\": optim.Adam\n",
    "# }\n",
    "C = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = nn.Parameter(torch.randn(1)) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        if input % 3 == 1:\n",
    "            out = C * self.x\n",
    "        else:\n",
    "            out = -self.x\n",
    "\n",
    "        if self.x < -1:\n",
    "            out += 10 * (self.x + 1) ** 2\n",
    "        elif self.x > 1:\n",
    "            out += 10 * (self.x - 1) ** 2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainval(model, optimizer, epochs=10):\n",
    "    # Train the model\n",
    "    train_loss = []\n",
    "    x_vals = [model.x.item()]\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # Forward pass\n",
    "        loss = model(epoch)\n",
    "            \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the training loss\n",
    "        train_loss.append(model.x.grad.item())\n",
    "        x_vals.append(model.x.item())\n",
    "\n",
    "    return train_loss, x_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "# else:\n",
    "#     device = torch.device('cpu')\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimizer: SGD\n",
      "Running optimizer: Adam\n",
      "Running optimizer: RMSprop\n",
      "Running optimizer: AdaGrad\n",
      "Running optimizer: AMSGrad\n"
     ]
    }
   ],
   "source": [
    "train_losses = {}\n",
    "x_vals = {}\n",
    "for optimizer_name in OPTIMIZERS:\n",
    "    print(\"Running optimizer:\", optimizer_name)\n",
    "    train_losses[optimizer_name] = []\n",
    "    x_vals[optimizer_name] = []\n",
    "    for seed in SEEDS:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Instantiate the logistic regression model\n",
    "        model = Model()\n",
    "        model.to(device)\n",
    "\n",
    "        # Define the optimizer and the loss function\n",
    "        if optimizer_name == \"AMSGrad\":\n",
    "            beta = 1 / (1 + C ** 2)\n",
    "            optimizer = OPTIMIZERS[optimizer_name](model.parameters(), betas=(0, beta), lr=0.1, amsgrad=True)\n",
    "        elif optimizer_name == \"Adam\":\n",
    "            beta = 1 / (1 + C ** 2)\n",
    "            optimizer = OPTIMIZERS[optimizer_name](model.parameters(), betas=(0, beta), lr=0.1)\n",
    "        else:\n",
    "            optimizer = OPTIMIZERS[optimizer_name](model.parameters(), lr=0.1)\n",
    "\n",
    "        train_loss, x_val = trainval(model, optimizer, epochs=EPOCHS)\n",
    "        train_losses[optimizer_name].append(train_loss)\n",
    "        x_vals[optimizer_name].append(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(EPOCHS)\n",
    "# x = np.tile(x, SEED_NUM)\n",
    "# for optimizer_name in OPTIMIZERS:\n",
    "#     y = np.concatenate(train_losses[optimizer_name])\n",
    "#     sns.lineplot(x=x, y=y, label=optimizer_name)\n",
    "# plt.title(\"Gradients\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Gradient\")\n",
    "# plt.savefig(f\"../results/special_grad_C{C}.pdf\")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(EPOCHS + 1)\n",
    "x = np.tile(x, SEED_NUM)\n",
    "for optimizer_name in OPTIMIZERS:\n",
    "    y = np.concatenate(x_vals[optimizer_name])\n",
    "    sns.lineplot(x=x, y=y, label=optimizer_name)\n",
    "plt.title(\"X value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.savefig(f\"../results/special_x_C{C}.pdf\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
